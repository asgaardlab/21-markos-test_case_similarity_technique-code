{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 1 for clustering similar test steps in natural language "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign test steps that are exactly the same (in terms of text) to the same cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics as st\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "from textwrap import wrap\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Queue\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize, TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "\n",
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import Word2Vec, Phrases, KeyedVectors\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute number of unique words in df\n",
    "def get_number_unique_words(df):\n",
    "    words_list = list()\n",
    "    test_steps = list(df[\"Steps\"])\n",
    "    for step in test_steps:\n",
    "        for word in step:\n",
    "            words_list.append(word)\n",
    "    number_unique_words = len(set(words_list))\n",
    "    return number_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute number of unique words in df ('test case name' field)\n",
    "def get_number_unique_words_name(df):\n",
    "    words_list = list()\n",
    "    test_names = list(df[\"Case_Name\"])\n",
    "    for name in test_names:\n",
    "        for word in name:\n",
    "            words_list.append(word)\n",
    "    number_unique_words = len(set(words_list))\n",
    "    return number_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get list of words that occur less than a certain number of times\n",
    "def get_word_frequency(df):\n",
    "    words_list = list()\n",
    "    test_steps = list(df[\"Steps\"])\n",
    "    for step in test_steps:\n",
    "        for word in step:\n",
    "            words_list.append(word)\n",
    "    unique_words_list = set(words_list)\n",
    "    word_occurrence_dict = {}\n",
    "    for each_word in unique_words_list:\n",
    "        word_occurrence_dict[each_word] = 0\n",
    "\n",
    "    for step in test_steps:\n",
    "        for word in step:\n",
    "            word_occurrence_dict[word] += 1\n",
    "            \n",
    "    ten_times_occurrence_words = list()\n",
    "    # get list of words that occur only once\n",
    "    for word, occurrence in word_occurrence_dict.items():\n",
    "        if occurrence < 2:\n",
    "            ten_times_occurrence_words.append(word)\n",
    "\n",
    "    return ten_times_occurrence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get list of words that occur less than a certain number of times ('test case name' field)\n",
    "def get_word_frequency_name(df):\n",
    "    words_list = list()\n",
    "    test_names = list(df[\"Case_Name\"])\n",
    "    for name in test_names:\n",
    "        for word in name:\n",
    "            words_list.append(word)\n",
    "    unique_words_list = set(words_list)\n",
    "    word_occurrence_dict = {}\n",
    "    for each_word in unique_words_list:\n",
    "        word_occurrence_dict[each_word] = 0\n",
    "\n",
    "    for name in test_names:\n",
    "        for word in name:\n",
    "            word_occurrence_dict[word] += 1\n",
    "            \n",
    "    ten_times_occurrence_words = list()\n",
    "    # get list of words that occur only once\n",
    "    for word, occurrence in word_occurrence_dict.items():\n",
    "        if occurrence < 2:\n",
    "            ten_times_occurrence_words.append(word)\n",
    "\n",
    "    return ten_times_occurrence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove problematic/mispelled words from vocabulary\n",
    "def remove_problematic_words(df):\n",
    "    number_unique_words = get_number_unique_words(df)\n",
    "    print(\"Number of unique words across all test steps: \", number_unique_words)\n",
    "    \n",
    "    # load file with problematic words that exist in the test data\n",
    "    problematic_words = open('word2vec_vocab_problematic.txt', 'r')\n",
    "    problematic_words_list = list()\n",
    "    for word in problematic_words:\n",
    "        problematic_words_list.append(word.lstrip().rstrip())\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        step = row[\"Steps\"]\n",
    "        df.loc[index][\"Steps\"] = [elem for elem in step if not elem in problematic_words_list]\n",
    "        \n",
    "    number_unique_words = get_number_unique_words(df)\n",
    "    print(\"Number of unique words across all test steps after removing problematic words: \", number_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fix problematic/mispelled words from vocabulary\n",
    "def fix_problematic_words(df):\n",
    "    number_unique_words = get_number_unique_words(df)\n",
    "    print(\"Number of unique words across all test steps: \", number_unique_words)\n",
    "    \n",
    "    # load file with problematic words that exist in the test data\n",
    "    problematic_words = open('word2vec_vocab_to_fix.txt', 'r')\n",
    "    problematic_words_dict = {}\n",
    "    for line in problematic_words:\n",
    "        full_line = line.split(':')\n",
    "        try:\n",
    "            problematic_words_dict[full_line[0]] = [x.replace('\\n', '') for x in full_line[1].split(',')]\n",
    "        except:\n",
    "            problematic_words_dict[full_line[0]] = full_line[1].replace('\\n', '')\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        step = row[\"Steps\"]\n",
    "        modified_step = list()\n",
    "        for word in step:\n",
    "            if word in problematic_words_dict:\n",
    "                modified_step.extend(problematic_words_dict[word])\n",
    "            else:\n",
    "                modified_step.append(word)\n",
    "        df.loc[index][\"Steps\"] = modified_step \n",
    "        \n",
    "    number_unique_words = get_number_unique_words(df)\n",
    "    print(\"Number of unique words across all test steps after fixing problematic words: \", number_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_clean_data(df):\n",
    "\n",
    "    # Preprocessing and clean test steps\n",
    "    print(\"Cleaning test step field...\")\n",
    "    \n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: re.sub(r'http\\S+', 'URL', x))\n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: re.sub('\\/[\\w-]*', '', x))\n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: re.sub(r'\\{[^)]*\\}', '', x))\n",
    "\n",
    "    # lowercase the step descriptions\n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: x.lower())\n",
    "\n",
    "    # remove digits and words with digits\n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: re.sub('\\w*\\d\\w*','', x))\n",
    "\n",
    "    # remove punctuations\n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x))\n",
    "\n",
    "    # remove extra spaces\n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: re.sub(' +',' ',x))\n",
    "\n",
    "    # tokenization\n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: TweetTokenizer().tokenize(x))\n",
    "    number_unique_words = get_number_unique_words(df)\n",
    "    print(\"Number of unique words across all test steps: \", number_unique_words)\n",
    "\n",
    "    remove_problematic_words(df)\n",
    "    fix_problematic_words(df)\n",
    "    \n",
    "    # stopword removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: [w for w in x if not w in stop_words])\n",
    "    number_unique_words = get_number_unique_words(df)\n",
    "    print(\"Number of unique words in test steps after stopword removal: \", number_unique_words)\n",
    "\n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: [lemmatizer.lemmatize(w) for w in x])\n",
    "    number_unique_words = get_number_unique_words(df)\n",
    "    print(\"Number of unique words in test steps after lemmatization: \", number_unique_words)\n",
    "\n",
    "    # remove words that occur less than 10 times\n",
    "    ten_times_occurrence_words = get_word_frequency(df)\n",
    "    print(\"Number of words that occurred less than 10 times in test steps: \", len(ten_times_occurrence_words))\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        current_test_step = row[\"Steps\"]\n",
    "        list_words_to_remove = list()\n",
    "        for word in current_test_step:\n",
    "            if word in ten_times_occurrence_words:\n",
    "                list_words_to_remove.append(word)\n",
    "\n",
    "        test_steps_df.loc[index][\"Steps\"] = [elem for elem in current_test_step if not elem in list_words_to_remove]\n",
    "\n",
    "    print(\"Dataset size after preprocessing: \" , df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and preprocess files with test cases and build dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data directory and list of xlsx files\n",
    "current_dir = os.getcwd() \n",
    "parent_dir = os.path.dirname(current_dir) + \"\\\\filtered_data\\\\\"\n",
    "xlsxfiles = [os.path.join(root, name)\n",
    "             for root, dirs, files in os.walk(parent_dir)\n",
    "             for name in files\n",
    "             if name.endswith((\".xlsx\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input data...\n",
      "Done!\n",
      "Shape of data =>  (15668, 5)\n"
     ]
    }
   ],
   "source": [
    "# Declare pandas df to be populated\n",
    "column_names = [\"Type\", \"Key\", \"Case_Name\", \"Step_ID\", \"Steps\"]\n",
    "test_steps_df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "# Index to add data to the df\n",
    "index_to_add = 0\n",
    "\n",
    "print(\"Reading input data...\")   \n",
    "for test_file in xlsxfiles:\n",
    "    # load data and iterate through it\n",
    "    test_data_df = pd.read_excel(test_file)\n",
    "    for index, row in test_data_df.iterrows():\n",
    "        current_type = row[\"Type\"]\n",
    "        current_key = row[\"Key\"]\n",
    "        current_name = row[\"Case_Name\"]\n",
    "        current_step_id = row[\"Step_ID\"]\n",
    "        current_steps = row[\"Steps\"]\n",
    "        test_steps_df.loc[index_to_add] = [current_type, current_key, current_name, current_step_id, current_steps]\n",
    "        index_to_add += 1\n",
    "\n",
    "print(\"Done!\")\n",
    "print(\"Shape of data => \", test_steps_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning test case name field...\n",
      "Dataset size before preprocessing:  (15668, 5)\n",
      "Number of unique words across all test names:  1519\n",
      "Number of unique words in test names after stopword removal:  1447\n",
      "Number of words that occurred only once in test case names:  164\n",
      "Number of unique words in test names in the end:  1138\n",
      "Dataset size after preprocessing:  (15668, 5)\n"
     ]
    }
   ],
   "source": [
    "# Call preprocessing function\n",
    "preprocess_clean_data(test_steps_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of list of tuples: 15668\n",
      "Length of list with test steps:  15668\n"
     ]
    }
   ],
   "source": [
    "# Build tuples with (step_id, step_text) - used to retrieve the step ID in the end (after the clustering) - and get only test steps for clustering\n",
    "step_id_text_tuple_list = list()\n",
    "test_steps_clustering_list = list()\n",
    "for index, row in test_steps_df.iterrows():\n",
    "    step_id = row[\"Step_ID\"]\n",
    "    step_text = row[\"Steps\"]\n",
    "    step_id_text_tuple_list.append((step_id,step_text))\n",
    "\n",
    "    temp_list = list()\n",
    "    if isinstance(row[\"Steps\"], list):\n",
    "        for elem in row[\"Steps\"]:\n",
    "            temp_list.append(elem)\n",
    "    else:\n",
    "        if isinstance(row[\"Steps\"], str):\n",
    "            temp_list.append(row[\"Steps\"])\n",
    "        \n",
    "    # Build list of lists of tokens (words)\n",
    "    test_steps_clustering_list.append(temp_list)\n",
    "    \n",
    "print(\"Length of list of tuples:\" , len(step_id_text_tuple_list))\n",
    "print(\"Length of list with test steps: \" , len(test_steps_clustering_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of list of tuples: 15644\n",
      "Length of list with test steps:  15644\n"
     ]
    }
   ],
   "source": [
    "# Remove empty steps\n",
    "index = 0\n",
    "steps_to_remove = list()\n",
    "for step in test_steps_clustering_list:\n",
    "    if len(step) == 0:\n",
    "        steps_to_remove.append(index)\n",
    "    index += 1\n",
    "\n",
    "step_id_text_tuple_list = [step_id_text_tuple_list[index] for index in range(len(step_id_text_tuple_list)) if not index in steps_to_remove]\n",
    "test_steps_clustering_list = [test_steps_clustering_list[index] for index in range(len(test_steps_clustering_list)) if not index in steps_to_remove]\n",
    "print(\"Length of list of tuples:\" , len(step_id_text_tuple_list))\n",
    "print(\"Length of list with test steps: \" , len(test_steps_clustering_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic stats for the test steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps :  15644\n",
      "Average number of words per test step:  3.925722321656865\n",
      "1742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['verify', 'validate', 'check', 'button', 'item']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of test steps\n",
    "print(\"Total number of steps : \", len(test_steps_training_list))\n",
    "\n",
    "# Average number of word per test step, name, and type together\n",
    "total_number_words_steps = sum([len(steps) for steps in test_steps_training_list])\n",
    "avg_words_per_step = total_number_words_steps/len(test_steps_training_list)\n",
    "print(\"Average number of words per test step: \", avg_words_per_step)\n",
    "\n",
    "# Most frequent words\n",
    "word_freq = defaultdict(int)\n",
    "for step in test_steps_training_list:\n",
    "    for word in step:\n",
    "        word_freq[str(word)] += 1\n",
    "sorted(word_freq, key=word_freq.get, reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare test step texts for similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_steps_list = []\n",
    "found_flag = [False] * len(test_steps_clustering_list)\n",
    "\n",
    "for i in range(len(test_steps_clustering_list)-1):\n",
    "    temp_set = set()\n",
    "    if not found_flag[i]:\n",
    "        temp_set.add(i)\n",
    "        found_flag[i] = True\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    for j in range(i+1, len(test_steps_clustering_list)):\n",
    "        if found_flag[j]:\n",
    "            continue\n",
    "        else:\n",
    "            if test_steps_clustering_list[i] == test_steps_clustering_list[j]:\n",
    "                temp_set.add(j)\n",
    "                found_flag[j] = True\n",
    "    same_steps_list.append(temp_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of clusters :  4407\n"
     ]
    }
   ],
   "source": [
    "print(\"Final number of clusters : \", len(same_steps_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# Example of cluster with more than 10 items\n",
    "for key in cluster_labels_dict:\n",
    "    if len(cluster_labels_dict[key]) > 10:\n",
    "        print(key)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clustered data\n",
    "path_save_data = \"baseline_1/baseline_1_clustered_data.txt\"\n",
    "out_cluster_file = open(path_save_data, \"a\")\n",
    "cluster_id = 0\n",
    "for elem in same_steps_list:\n",
    "    for index in elem:\n",
    "        str_to_save = \"[\" + str(cluster_id) + \"]:\\t\\t\" + test_steps_df.loc[index][\"Key\"] + \"\\t\\t\" + str(step_id_text_tuple_list[index][0]) + \"\\t\\t\" + str(test_steps_clustering_list[index]) + \"\\n\"\n",
    "        out_cluster_file.write(str_to_save)\n",
    "    cluster_id += 1\n",
    "out_cluster_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cluster labels (step IDs)\n",
    "path_save_labels = \"baseline_1/baseline_1_cluster_labels.txt\"\n",
    "out_cluster_file = open(path_save_labels, \"a\")\n",
    "cluster_id = 0\n",
    "for elem in same_steps_list:\n",
    "    str_to_save = \"[\" + str(cluster_id) + \"]: \" + ','.join(str(step_id_text_tuple_list[x][0]) for x in elem) + \"\\n\"\n",
    "    out_cluster_file.write(str_to_save)\n",
    "    cluster_id += 1\n",
    "out_cluster_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute F-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load ground truth of similar test steps (to compute F-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read excel files with manually clustered samples\n",
    "manual_sample_dir = 'sample_manual_ground_truth/clusters/'\n",
    "sample_files = os.listdir(manual_sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_clusters_dict = {}\n",
    "for sample in sample_files:\n",
    "    sample_df = pd.read_excel(manual_sample_dir + sample)\n",
    "    for index, row in sample_df.iterrows():\n",
    "        cluster_id = row['cluster_id']\n",
    "        step_id = row['step_id']\n",
    "        if step_id in manual_clusters_dict:\n",
    "            existing_list = manual_clusters_dict[step_id]\n",
    "            existing_list.append(cluster_id)\n",
    "            manual_clusters_dict[step_id] = existing_list\n",
    "        else:\n",
    "            manual_clusters_dict[step_id] = [cluster_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test step samples which were manually clustered:  394\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of test step samples which were manually clustered: \", len(manual_clusters_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_steps_to_evaluate_list = list(manual_clusters_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "appr_clusters_dict = {}\n",
    "cluster_id = 0\n",
    "for elem in same_steps_list:\n",
    "    for index in elem:\n",
    "        step_id = step_id_text_tuple_list[index][0]\n",
    "        appr_clusters_dict[int(step_id)] = cluster_id  \n",
    "    cluster_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test steps which were clustered by the approach:  15644\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of test steps which were clustered by the approach: \", len(appr_clusters_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare and initialize variables to compute F-score\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through list of test steps to evaluate\n",
    "wrong = []\n",
    "for i in range(len(test_steps_to_evaluate_list)-1):\n",
    "    for j in range(i+1, len(test_steps_to_evaluate_list)):\n",
    "        step_id_1 = test_steps_to_evaluate_list[i]\n",
    "        step_id_2 = test_steps_to_evaluate_list[j]\n",
    "        \n",
    "        # true positive case\n",
    "        if (manual_clusters_dict[step_id_1] == manual_clusters_dict[step_id_2]) and (appr_clusters_dict[step_id_1] == appr_clusters_dict[step_id_2]):\n",
    "            TP += 1\n",
    "            \n",
    "        # false positive case\n",
    "        elif (manual_clusters_dict[step_id_1] != manual_clusters_dict[step_id_2]) and (appr_clusters_dict[step_id_1] == appr_clusters_dict[step_id_2]):\n",
    "            FP += 1\n",
    "            \n",
    "        # false negative case\n",
    "        elif (manual_clusters_dict[step_id_1] == manual_clusters_dict[step_id_2]) and (appr_clusters_dict[step_id_1] != appr_clusters_dict[step_id_2]):\n",
    "            FN += 1\n",
    "            wrong.append((step_id_1,step_id_2))\n",
    "            \n",
    "        # true negative case\n",
    "        elif (manual_clusters_dict[step_id_1] != manual_clusters_dict[step_id_2]) and (appr_clusters_dict[step_id_1] != appr_clusters_dict[step_id_2]):\n",
    "            TN += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision =  1.0\n",
      "Recall =  0.5432098765432098\n"
     ]
    }
   ],
   "source": [
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score =  0.704\n"
     ]
    }
   ],
   "source": [
    "f_score = (2 * precision * recall) / (precision + recall)\n",
    "print(\"F-score = \", f_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
