{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 3 for test case similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Technique 3 uses the cosine score to compute the similarity between numerical vectors representing test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics as st\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize, TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load clusters obtained by the best approach (ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results_dir = 'experiments/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "approach_ensemble_dir = experiment_results_dir + 'results_approach_ensemble/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictionary to indicate the cluster ID of each test step\n",
    "approach_ensemble_dict = {}\n",
    "cluster_file = open(approach_ensemble_dir + 'ensemble_cluster_labels.txt')\n",
    "for line in cluster_file:\n",
    "    full_line = line.split()\n",
    "    cluster_id = int(full_line[0].replace('[', '').replace(']', '').replace(':', ''))\n",
    "    step_id_list = full_line[1].split(',')\n",
    "    for step_id in step_id_list:\n",
    "        approach_ensemble_dict[int(step_id)] = cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test steps which were clustered by the approach:  15644\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of test steps which were clustered by the approach: \", len(approach_ensemble_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute number of unique words in df\n",
    "def get_number_unique_words(df):\n",
    "    words_list = list()\n",
    "    test_steps = list(df[\"Steps\"])\n",
    "    for step in test_steps:\n",
    "        for word in step:\n",
    "            words_list.append(word)\n",
    "    number_unique_words = len(set(words_list))\n",
    "    return number_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get list of words that occur less than a certain number of times\n",
    "def get_word_frequency(df):\n",
    "    words_list = list()\n",
    "    test_steps = list(df[\"Steps\"])\n",
    "    for step in test_steps:\n",
    "        for word in step:\n",
    "            words_list.append(word)\n",
    "    unique_words_list = set(words_list)\n",
    "    word_occurrence_dict = {}\n",
    "    for each_word in unique_words_list:\n",
    "        word_occurrence_dict[each_word] = 0\n",
    "\n",
    "    for step in test_steps:\n",
    "        for word in step:\n",
    "            word_occurrence_dict[word] += 1\n",
    "            \n",
    "    ten_times_occurrence_words = list()\n",
    "    # get list of words that occur only once\n",
    "    for word, occurrence in word_occurrence_dict.items():\n",
    "        if occurrence < 2:\n",
    "            ten_times_occurrence_words.append(word)\n",
    "\n",
    "    return ten_times_occurrence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove problematic/mispelled words from vocabulary\n",
    "def remove_problematic_words(df):\n",
    "    number_unique_words = get_number_unique_words(df)\n",
    "    print(\"Number of unique words across all test steps: \", number_unique_words)\n",
    "    \n",
    "    # load file with problematic words that exist in the test data\n",
    "    problematic_words = open('word2vec_vocab_problematic.txt', 'r')\n",
    "    problematic_words_list = list()\n",
    "    for word in problematic_words:\n",
    "        problematic_words_list.append(word.lstrip().rstrip())\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        step = row[\"Steps\"]\n",
    "        df.loc[index][\"Steps\"] = [elem for elem in step if not elem in problematic_words_list]\n",
    "        \n",
    "    number_unique_words = get_number_unique_words(df)\n",
    "    print(\"Number of unique words across all test steps after removing problematic words: \", number_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fix problematic/mispelled words from vocabulary\n",
    "def fix_problematic_words(df):\n",
    "    number_unique_words = get_number_unique_words(df)\n",
    "    print(\"Number of unique words across all test steps: \", number_unique_words)\n",
    "    \n",
    "    # load file with problematic words that exist in the test data\n",
    "    problematic_words = open('word2vec_vocab_to_fix.txt', 'r')\n",
    "    problematic_words_dict = {}\n",
    "    for line in problematic_words:\n",
    "        full_line = line.split(':')\n",
    "        try:\n",
    "            problematic_words_dict[full_line[0]] = [x.replace('\\n', '') for x in full_line[1].split(',')]\n",
    "        except:\n",
    "            problematic_words_dict[full_line[0]] = full_line[1].replace('\\n', '')\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        step = row[\"Steps\"]\n",
    "        modified_step = list()\n",
    "        for word in step:\n",
    "            if word in problematic_words_dict:\n",
    "                modified_step.extend(problematic_words_dict[word])\n",
    "            else:\n",
    "                modified_step.append(word)\n",
    "        df.loc[index][\"Steps\"] = modified_step \n",
    "        \n",
    "    number_unique_words = get_number_unique_words(df)\n",
    "    print(\"Number of unique words across all test steps after fixing problematic words: \", number_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_clean_data(df):\n",
    "\n",
    "    # Preprocessing and clean test steps\n",
    "    print(\"Cleaning test step field...\")\n",
    "    \n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: re.sub(r'http\\S+', 'URL', x))\n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: re.sub('\\/[\\w-]*', '', x))\n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: re.sub(r'\\{[^)]*\\}', '', x))\n",
    "\n",
    "    # lowercase the step descriptions\n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: x.lower())\n",
    "\n",
    "    # remove digits and words with digits\n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: re.sub('\\w*\\d\\w*','', x))\n",
    "\n",
    "    # remove punctuations\n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x))\n",
    "\n",
    "    # remove extra spaces\n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: re.sub(' +',' ',x))\n",
    "\n",
    "    # tokenization\n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: TweetTokenizer().tokenize(x))\n",
    "    number_unique_words = get_number_unique_words(df)\n",
    "    print(\"Number of unique words across all test steps: \", number_unique_words)\n",
    "\n",
    "    remove_problematic_words(df)\n",
    "    fix_problematic_words(df)\n",
    "    \n",
    "    # stopword removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: [w for w in x if not w in stop_words])\n",
    "    number_unique_words = get_number_unique_words(df)\n",
    "    print(\"Number of unique words in test steps after stopword removal: \", number_unique_words)\n",
    "\n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    df[\"Steps\"] = df[\"Steps\"].apply(lambda x: [lemmatizer.lemmatize(w) for w in x])\n",
    "    number_unique_words = get_number_unique_words(df)\n",
    "    print(\"Number of unique words in test steps after lemmatization: \", number_unique_words)\n",
    "\n",
    "    # remove words that occur less than 10 times\n",
    "    ten_times_occurrence_words = get_word_frequency(df)\n",
    "    print(\"Number of words that occurred less than 10 times in test steps: \", len(ten_times_occurrence_words))\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        current_test_step = row[\"Steps\"]\n",
    "        list_words_to_remove = list()\n",
    "        for word in current_test_step:\n",
    "            if word in ten_times_occurrence_words:\n",
    "                list_words_to_remove.append(word)\n",
    "\n",
    "        test_steps_df.loc[index][\"Steps\"] = [elem for elem in current_test_step if not elem in list_words_to_remove]\n",
    "\n",
    "    print(\"Dataset size after preprocessing: \" , df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and preprocess files with test cases and build dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data directory and list of xlsx files\n",
    "current_dir = os.getcwd() \n",
    "parent_dir = os.path.dirname(current_dir) + \"\\\\filtered_data\\\\\"\n",
    "xlsxfiles = [os.path.join(root, name)\n",
    "             for root, dirs, files in os.walk(parent_dir)\n",
    "             for name in files\n",
    "             if name.endswith((\".xlsx\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare pandas df to be populated\n",
    "column_names = [\"Type\", \"Key\", \"Case_Name\", \"Step_ID\", \"Steps\"]\n",
    "test_steps_df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "# Index to add data to the df\n",
    "index_to_add = 0\n",
    "\n",
    "print(\"Reading input data...\")   \n",
    "for test_file in xlsxfiles:\n",
    "    # load data and iterate through it\n",
    "    test_data_df = pd.read_excel(test_file)\n",
    "    for index, row in test_data_df.iterrows():\n",
    "        current_type = row[\"Type\"]\n",
    "        current_key = row[\"Key\"]\n",
    "        current_name = row[\"Case_Name\"]\n",
    "        current_step_id = row[\"Step_ID\"]\n",
    "        current_steps = row[\"Steps\"]\n",
    "        test_steps_df.loc[index_to_add] = [current_type, current_key, current_name, current_step_id, current_steps]\n",
    "        index_to_add += 1\n",
    "\n",
    "print(\"Done!\")\n",
    "print(\"Shape of data => \", test_steps_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call preprocessing function\n",
    "preprocess_clean_data(test_steps_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tuples with (step_id, step_text) - used to retrieve the step ID in the end (after the clustering) - and get only test steps for clustering\n",
    "step_id_text_tuple_list = list()\n",
    "test_steps_clustering_list = list()\n",
    "for index, row in test_steps_df.iterrows():\n",
    "    step_id = row[\"Step_ID\"]\n",
    "    step_text = row[\"Steps\"]\n",
    "    step_id_text_tuple_list.append((step_id,step_text))\n",
    "\n",
    "    temp_list = list()\n",
    "    if isinstance(row[\"Steps\"], list):\n",
    "        for elem in row[\"Steps\"]:\n",
    "            temp_list.append(elem)\n",
    "    else:\n",
    "        if isinstance(row[\"Steps\"], str):\n",
    "            temp_list.append(row[\"Steps\"])\n",
    "        \n",
    "    # Build list of lists of tokens (words)\n",
    "    test_steps_clustering_list.append(temp_list)\n",
    "    \n",
    "print(\"Length of list of tuples:\" , len(step_id_text_tuple_list))\n",
    "print(\"Length of list with test steps: \" , len(test_steps_clustering_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty steps\n",
    "index = 0\n",
    "steps_to_remove = list()\n",
    "for step in test_steps_clustering_list:\n",
    "    if len(step) == 0:\n",
    "        steps_to_remove.append(index)\n",
    "    index += 1\n",
    "\n",
    "step_id_text_tuple_list = [step_id_text_tuple_list[index] for index in range(len(step_id_text_tuple_list)) if not index in steps_to_remove]\n",
    "test_steps_clustering_list = [test_steps_clustering_list[index] for index in range(len(test_steps_clustering_list)) if not index in steps_to_remove]\n",
    "print(\"Length of list of tuples:\" , len(step_id_text_tuple_list))\n",
    "print(\"Length of list with test steps: \" , len(test_steps_clustering_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build numeric matrix of [test_cases] x [clusters] to indicate which clusters are related to each test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test cases:  3323\n"
     ]
    }
   ],
   "source": [
    "test_case_steps_dict = {}\n",
    "for index, row in test_steps_df.iterrows():\n",
    "    test_case_key = row['Key']\n",
    "    test_step_id = row['Step_ID']\n",
    "    test_steps = row['Steps']\n",
    "    if len(test_steps) == 0:\n",
    "        continue\n",
    "    if test_case_key in test_case_steps_dict:\n",
    "        existing_list = test_case_steps_dict[test_case_key]\n",
    "        existing_list.append(test_step_id)\n",
    "        test_case_steps_dict[test_case_key] = existing_list\n",
    "    else:\n",
    "        test_case_steps_dict[test_case_key] = [test_step_id]\n",
    "print(\"Number of test cases: \", len(test_case_steps_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_matrix = np.zeros((len(test_case_steps_dict),number_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = 0\n",
    "for test_case_key in test_case_steps_dict:\n",
    "    steps_ids_list = test_case_steps_dict[test_case_key]\n",
    "    cluster_ids_to_fill_list = list()\n",
    "    for each_step_id in steps_ids_list:\n",
    "        cluster_id = approach_ensemble_dict[each_step_id]\n",
    "        cluster_ids_to_fill_list.append(cluster_id)\n",
    "    \n",
    "    # Fill in matrix with specific column indices (cluster ids)\n",
    "    tuple_count_clusters = list()\n",
    "    cluster_ids_set = set(cluster_ids_to_fill_list)\n",
    "    for elem in cluster_ids_set:\n",
    "        counter = 0\n",
    "        for cluster_id in cluster_ids_to_fill_list:\n",
    "            if cluster_id == elem:\n",
    "                counter += 1\n",
    "        tuple_count_clusters.append((elem,counter))\n",
    "\n",
    "    first_tuple_elements = [a_tuple[0] for a_tuple in tuple_count_clusters] # indices of columns to be filled in\n",
    "    second_tuple_elements = [a_tuple[1] for a_tuple in tuple_count_clusters] # number of steps in each cluster (Data to be filled in)\n",
    "\n",
    "    numeric_matrix[row_index,first_tuple_elements] = second_tuple_elements\n",
    "    row_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(len(test_case_steps_dict)):\n",
    "    for col in range(number_clusters):\n",
    "        if numeric_matrix[row,col] > 1:\n",
    "            print(numeric_matrix[row,col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cosine similarity score and build similarity matrix with this score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize matrix of scores\n",
    "dist_matrix = np.zeros((len(test_case_steps_dict),len(test_case_steps_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case_dict_keys_list = list(test_case_steps_dict.keys())\n",
    "for i in range(len(test_case_steps_dict)):\n",
    "    for j in range(i, len(test_case_steps_dict)):\n",
    "        computed_dist = 1 - spatial.distance.cosine(numeric_matrix[i,:], numeric_matrix[j,:])\n",
    "        dist_matrix[i,j] = dist_matrix[j,i] = computed_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform search of different thresholds for the cosine score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_thresholds = [np.around(x, 2) for x in np.arange(0.1, 1.05, 0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing threshold :  0.1\n",
      "Number of groups of similar test cases:  291\n",
      "Number of test cases that have at least another similar case:  2725\n",
      "Number of test cases that do NOT have any similar case:  598\n",
      "Analyzing threshold :  0.15\n",
      "Number of groups of similar test cases:  301\n",
      "Number of test cases that have at least another similar case:  2709\n",
      "Number of test cases that do NOT have any similar case:  614\n",
      "Analyzing threshold :  0.2\n",
      "Number of groups of similar test cases:  305\n",
      "Number of test cases that have at least another similar case:  2690\n",
      "Number of test cases that do NOT have any similar case:  633\n",
      "Analyzing threshold :  0.25\n",
      "Number of groups of similar test cases:  318\n",
      "Number of test cases that have at least another similar case:  2674\n",
      "Number of test cases that do NOT have any similar case:  649\n",
      "Analyzing threshold :  0.3\n",
      "Number of groups of similar test cases:  330\n",
      "Number of test cases that have at least another similar case:  2634\n",
      "Number of test cases that do NOT have any similar case:  689\n",
      "Analyzing threshold :  0.35\n",
      "Number of groups of similar test cases:  340\n",
      "Number of test cases that have at least another similar case:  2585\n",
      "Number of test cases that do NOT have any similar case:  738\n",
      "Analyzing threshold :  0.4\n",
      "Number of groups of similar test cases:  351\n",
      "Number of test cases that have at least another similar case:  2545\n",
      "Number of test cases that do NOT have any similar case:  778\n",
      "Analyzing threshold :  0.45\n",
      "Number of groups of similar test cases:  364\n",
      "Number of test cases that have at least another similar case:  2502\n",
      "Number of test cases that do NOT have any similar case:  821\n",
      "Analyzing threshold :  0.5\n",
      "Number of groups of similar test cases:  378\n",
      "Number of test cases that have at least another similar case:  2489\n",
      "Number of test cases that do NOT have any similar case:  834\n",
      "Analyzing threshold :  0.55\n",
      "Number of groups of similar test cases:  391\n",
      "Number of test cases that have at least another similar case:  2426\n",
      "Number of test cases that do NOT have any similar case:  897\n",
      "Analyzing threshold :  0.6\n",
      "Number of groups of similar test cases:  403\n",
      "Number of test cases that have at least another similar case:  2384\n",
      "Number of test cases that do NOT have any similar case:  939\n",
      "Analyzing threshold :  0.65\n",
      "Number of groups of similar test cases:  408\n",
      "Number of test cases that have at least another similar case:  2367\n",
      "Number of test cases that do NOT have any similar case:  956\n",
      "Analyzing threshold :  0.7\n",
      "Number of groups of similar test cases:  410\n",
      "Number of test cases that have at least another similar case:  2325\n",
      "Number of test cases that do NOT have any similar case:  998\n",
      "Analyzing threshold :  0.75\n",
      "Number of groups of similar test cases:  409\n",
      "Number of test cases that have at least another similar case:  2228\n",
      "Number of test cases that do NOT have any similar case:  1095\n",
      "Analyzing threshold :  0.8\n",
      "Number of groups of similar test cases:  406\n",
      "Number of test cases that have at least another similar case:  2179\n",
      "Number of test cases that do NOT have any similar case:  1144\n",
      "Analyzing threshold :  0.85\n",
      "Number of groups of similar test cases:  419\n",
      "Number of test cases that have at least another similar case:  2116\n",
      "Number of test cases that do NOT have any similar case:  1207\n",
      "Analyzing threshold :  0.9\n",
      "Number of groups of similar test cases:  433\n",
      "Number of test cases that have at least another similar case:  2043\n",
      "Number of test cases that do NOT have any similar case:  1280\n",
      "Analyzing threshold :  0.95\n",
      "Number of groups of similar test cases:  445\n",
      "Number of test cases that have at least another similar case:  1977\n",
      "Number of test cases that do NOT have any similar case:  1346\n",
      "Analyzing threshold :  1.0\n",
      "Number of groups of similar test cases:  432\n",
      "Number of test cases that have at least another similar case:  1915\n",
      "Number of test cases that do NOT have any similar case:  1408\n"
     ]
    }
   ],
   "source": [
    "for threshold in similarity_thresholds:\n",
    "    print(\"Analyzing threshold : \" , threshold)\n",
    "    \n",
    "    # Identify pairs of test cases that are similar\n",
    "    test_cases_overlap_tuple_list = list()\n",
    "    for i in range(len(dist_matrix)-1):\n",
    "        for j in range(i+1, len(dist_matrix)):\n",
    "            if (dist_matrix[i,j] >= threshold): \n",
    "                test_cases_overlap_tuple_list.append((i,j))\n",
    "            \n",
    "    # Merge test cases that are similar\n",
    "    similar_test_cases_list = list()\n",
    "    for test_case_tuple in test_cases_overlap_tuple_list:\n",
    "        index_1 = test_case_tuple[0]\n",
    "        index_2 = test_case_tuple[1]\n",
    "        found = False\n",
    "        for test_case_set in similar_test_cases_list:\n",
    "            if (index_1 in test_case_set) or (index_2 in test_case_set):\n",
    "                test_case_set.add(index_1)\n",
    "                test_case_set.add(index_2)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            temp_set = set()\n",
    "            temp_set.add(index_1)\n",
    "            temp_set.add(index_2)\n",
    "            similar_test_cases_list.append(temp_set)\n",
    "    print(\"Number of groups of similar test cases: \", len(similar_test_cases_list))\n",
    "\n",
    "    test_case_key_unique = list()\n",
    "    for elem in similar_test_cases_list:\n",
    "        for index in elem:\n",
    "            if index not in test_case_key_unique:\n",
    "                test_case_key_unique.append(index)\n",
    "    print(\"Number of test cases that have at least another similar case: \", len(test_case_key_unique))\n",
    "    print(\"Number of test cases that do NOT have any similar case: \", ( len(test_case_steps_dict) - len(test_case_key_unique) ))\n",
    "\n",
    "    approach_3_dict = {}\n",
    "    cluster_id = 0\n",
    "    for each_set in similar_test_cases_list:\n",
    "        for elem in each_set:\n",
    "            case_key = test_case_dict_keys_list[elem]\n",
    "            approach_3_dict[case_key] = cluster_id\n",
    "        cluster_id += 1\n",
    "\n",
    "    for elem in range(len(test_case_steps_dict)):\n",
    "        if elem not in test_case_key_unique:\n",
    "            case_key = test_case_dict_keys_list[elem]\n",
    "            approach_3_dict[case_key] = cluster_id\n",
    "            cluster_id += 1\n",
    "\n",
    "    # save groups of similar test cases\n",
    "    file_name = 'test_case_similarity/results_approach_3/approach_3_similar_test_cases_' + str(threshold) + '.txt'\n",
    "    output_file = open(file_name, 'w')\n",
    "\n",
    "    counter = 0\n",
    "    for key in approach_3_dict:\n",
    "        output_file.write(key + \":\" + str(approach_3_dict[key]) + \"\\n\")\n",
    "    output_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
